import os
from pathlib import Path
import pandas as pd
from typing import List, Dict
from omegaconf import DictConfig
import numpy as np
import torch


"""
A class to load embeddings for training and validation dataset splits.
"""


class EmbsLoader:
    """
    A specialized EmbsLoader for embeddings generated by the embedder mode.
    These embeddings are stored in eval/results/embeddings/{model_name}/{model_name}_{split}.npy
    """
    
    def __init__(self, cfg: DictConfig):
        self.embs_space = cfg.test.emb_space
        # Get the project root directory (2 levels up from this file: eval/utils.py -> project_root)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.embeddings_dir = os.path.join(project_root, "eval", "results", "embeddings", self.embs_space)
        print(f'Embedding space: {self.embs_space}')
        print(f'Embeddings directory: {self.embeddings_dir}')
    
    def __load_embeddings(self, file_name: str) -> Dict:
        file_path = os.path.join(self.embeddings_dir, file_name)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Embedding file not found: {file_path}")
        return np.load(file_path, allow_pickle=True).item()
    
    def get_train_embeddings(self) -> Dict:
        return self.__load_embeddings(f'{self.embs_space}_train.npy')
    
    def get_val_embeddings(self) -> Dict:
        return self.__load_embeddings(f'{self.embs_space}_val.npy')
    
    def get_test_embeddings(self) -> Dict:
        return self.__load_embeddings(f'{self.embs_space}_test.npy')


def create_preds_df(data: List[Dict]):
    """
    Converts prediction data to a pandas dataframe with specified top-k predictions and probabilities.

    Args:
        data (List[Dict]): A list of dictionaries containing prediction data.
    """
    if isinstance(data, list):
        if isinstance(data[0], dict):
            df = pd.DataFrame(data)
        else:
            raise ValueError("Data must be a list of dictionaries")
    else:
        raise ValueError("Data must be a list")

    return df


def save_predictions_csv(data: pd.DataFrame, directory: str = 'results', filename: str = 'out'):
    """
    Saves prediction data from pandas dataframe to a .csv file.

    Args:
        data (pd.DataFrame): A dataframe containing prediction data.
        directory (str): The directory to save the CSV file.
        filename (str): Name of the CSV file to be saved in the 'results' directory.
    """
    # Handle different directory path formats
    if directory.startswith('eval/'):
        # If directory starts with 'eval/', resolve from project root
        project_root = Path(__file__).parent.parent  # Go up from eval/ to project root
        parent_dir = project_root / directory
    elif os.path.isabs(directory):
        # If absolute path, use as-is
        parent_dir = Path(directory)
    else:
        # Relative path from eval/ directory
        parent_dir = Path(__file__).parent / directory
    
    if not parent_dir.exists():
        parent_dir.mkdir(parents=True, exist_ok=True)
    output_path = parent_dir / f'{filename}.csv'
    data.to_csv(output_path, index=False)


def eval_on_clean_labels(df, clean_labels, verbose=True):
    merged = pd.merge(clean_labels, df, left_on='id', right_on='img_id')
    merged['top_1_pred'] = merged['top_1_pred'].astype(str)

    merged['correct_new'] = merged.apply(
        lambda row: row['top_1_pred'].strip() in str(row['proposed_labels']).strip().split(", ")
        if "," in str(row['proposed_labels'])
        else row['top_1_pred'].strip() == str(row['proposed_labels']).strip(),
        axis=1
    )

    merged['correct_orig'] = merged['top_1_pred'] == merged['original_label_x']

    if verbose:
        print("Accuracy on 'clean' labels: ", np.round(merged['correct_new'].mean() * 100, 2))
    return merged


def save_accuracy_results_csv(predictions_df: pd.DataFrame, clean_labels_path: str, directory: str = 'results', filename: str = 'out', project_root=None, verbose=True):
    """
    Saves accuracy results to CSV files including validation accuracy and clean validation accuracy.
    
    Args:
        predictions_df (pd.DataFrame): DataFrame containing model predictions
        clean_labels_path (str): Path to the clean validation labels CSV file
        directory (str): Directory to save the CSV files (relative to eval/ folder)
        filename (str): Base filename for the CSV files
        project_root: Path to project root. If None, will be inferred from this file's location
        verbose: Whether to print verbose output (default: True)
    """
    try:
        if project_root is None:
            project_root = Path(__file__).parent.parent
        
        # Load clean validation labels
        clean_labels = pd.read_csv(clean_labels_path)
        
        # Calculate validation accuracy (top-1 accuracy on original labels)
        if 'top_1_pred' in predictions_df.columns and 'original_label' in predictions_df.columns:
            validation_accuracy = (predictions_df['top_1_pred'] == predictions_df['original_label']).mean() * 100
        else:
            validation_accuracy = 0.0
            print("Warning: Could not calculate validation accuracy - missing required columns")
        
        # Calculate clean validation accuracy
        try:
            clean_results = eval_on_clean_labels(predictions_df, clean_labels, verbose=verbose)
            clean_validation_accuracy = clean_results['correct_new'].mean() * 100
        except Exception as e:
            print(f"Warning: Could not calculate clean validation accuracy: {e}")
            clean_validation_accuracy = 0.0
        
        # Create accuracy results DataFrame
        accuracy_data = {
            'Metric': ['Validation', 'Cleaner Validation'],
            'Accuracy (%)': [round(validation_accuracy, 2), round(clean_validation_accuracy, 2)]
        }
        
        accuracy_df = pd.DataFrame(accuracy_data)
        
        # Save accuracy results CSV - handle directory path correctly
        if directory.startswith('eval/'):
            # If directory already starts with 'eval/', use it directly from project root
            save_dir = project_root / directory
        else:
            # Otherwise, add 'eval/' prefix
            save_dir = project_root / 'eval' / directory
        save_dir.mkdir(parents=True, exist_ok=True)
        accuracy_output_path = save_dir / f'{filename}_accuracy.csv'
        accuracy_df.to_csv(accuracy_output_path, index=False)
        if verbose:
            print(f'Accuracy results saved to: {accuracy_output_path}')
        
        return validation_accuracy, clean_validation_accuracy
        
    except Exception as e:
        print(f"Error saving accuracy results: {e}")
        return 0.0, 0.0


def count_parameters_simple(model):
    """Count parameters using simple PyTorch parameter counting."""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def save_embeddings_to_npy(results, model_name, dataloader_name, convert_labels_to_int=True, project_root=None):
    """
    Save embeddings results to NPY file. This function can be used by all embedder models
    to avoid code duplication.
    
    Args:
        results: List of results from model testing, can be nested lists
        model_name: Name of the model (used for directory and filename)
        dataloader_name: Name of the dataloader (used for filename)
        convert_labels_to_int: Whether to convert labels to integers (default: True)
        project_root: Path to project root. If None, will be inferred from this file's location
    """
    if project_root is None:
        project_root = Path(__file__).parent.parent
    
    # Flatten nested results if needed
    flattened_results = []
    for sublist in results:
        if isinstance(sublist, list):
            flattened_results.extend(sublist)
        else:
            flattened_results.append(sublist)
    
    labels, image_names, embeddings = [], [], []

    for output in flattened_results:
        # Handle tensor conversion only if torch is available
        if torch is not None and torch.is_tensor(output[0]):
            batch_labels = output[0].cpu().numpy()
        else:
            batch_labels = output[0]
            
        batch_image_names = output[1]
        
        if torch is not None and torch.is_tensor(output[2]):
            batch_embeddings = output[2].cpu().numpy()
        else:
            batch_embeddings = output[2]

        if convert_labels_to_int:
            labels.extend([int(label) for label in batch_labels])
        else:
            labels.extend(batch_labels)
        image_names.extend(batch_image_names)
        embeddings.append(batch_embeddings)

    embeddings = np.vstack(embeddings)
    data = {'label': labels, 'image_name': image_names, 'embedding': embeddings}

    # Print detailed info matching classifier style
    print(f"Processing embeddings: {len(data['label'])} images, {len(data['image_name'])} filenames, embedding shape: {data['embedding'].shape}")

    # Save to /eval/results/embeddings/{model_name}/ directory structure
    save_dir = project_root / 'eval' / 'results' / 'embeddings' / model_name
    save_dir.mkdir(parents=True, exist_ok=True)
    save_path = save_dir / f'{model_name}_{dataloader_name}.npy'
    np.save(save_path, data)
    print(f'âœ“ Embeddings data saved to: {save_path}')
    
    return save_path


def calculate_knn_accuracy(predictions_df: pd.DataFrame, clean_labels_path: str = None, k_values: List[int] = None):
    """
    Calculate KNN accuracy for different k values from predictions DataFrame.
    
    Args:
        predictions_df (pd.DataFrame): DataFrame containing predictions with k_X_pred columns
        clean_labels_path (str, optional): Path to clean labels CSV for cleaner accuracy calculation
        k_values (List[int], optional): List of k values to calculate accuracy for. 
                                       If None, will auto-detect from columns.
    
    Returns:
        pd.DataFrame: DataFrame with columns ['k', 'accuracy', 'cleaner_accuracy']
    """
    import re
    
    if k_values is None:
        # Auto-detect k values from column names
        k_columns = [col for col in predictions_df.columns if col.startswith('k_') and col.endswith('_pred')]
        k_values = []
        for col in k_columns:
            match = re.search(r'k_(\d+)_pred', col)
            if match:
                k_values.append(int(match.group(1)))
        k_values = sorted(k_values)
    
    def calculate_accuracy(df, pred_col):
        """Calculate accuracy for a given prediction column."""
        if pred_col not in df.columns:
            return 0.0
        
        # Filter out rows where prediction is None/NaN
        valid_predictions = df[df[pred_col].notna()]
        if len(valid_predictions) == 0:
            return 0.0
            
        correct_predictions = (valid_predictions['original_label'] == valid_predictions[pred_col]).sum()
        total_predictions = len(valid_predictions)
        accuracy = 100 * (correct_predictions / total_predictions) if total_predictions > 0 else 0
        return round(accuracy, 2)
    
    def calculate_cleaner_accuracy(df, pred_col, clean_labels):
        """Calculate accuracy using cleaner labels for a given prediction column."""
        if pred_col not in df.columns:
            return 0.0
            
        try:
            # Merge with clean labels
            merged = pd.merge(clean_labels, df, left_on='id', right_on='img_id')
            merged[pred_col] = merged[pred_col].astype(str)
            
            # Filter out rows where prediction is None/NaN
            valid_predictions = merged[merged[pred_col].notna()]
            if len(valid_predictions) == 0:
                return 0.0
            
            # Check if prediction is in proposed labels
            valid_predictions['correct_clean'] = valid_predictions.apply(
                lambda row: row[pred_col].strip() in str(row['proposed_labels']).strip().split(", ")
                if "," in str(row['proposed_labels'])
                else row[pred_col].strip() == str(row['proposed_labels']).strip(),
                axis=1
            )
            
            cleaner_accuracy = valid_predictions['correct_clean'].mean() * 100
            return round(cleaner_accuracy, 2)
        except Exception as e:
            print(f"Warning: Could not calculate cleaner accuracy for {pred_col}: {e}")
            return 0.0
    
    # Load clean labels if path provided
    clean_labels = None
    if clean_labels_path and os.path.exists(clean_labels_path):
        try:
            clean_labels = pd.read_csv(clean_labels_path)
        except Exception as e:
            print(f"Warning: Could not load clean labels from {clean_labels_path}: {e}")
    
    # Calculate accuracies for each k value
    results = []
    for k in k_values:
        pred_col = f'k_{k}_pred'
        accuracy = calculate_accuracy(predictions_df, pred_col)
        
        cleaner_accuracy = 0.0
        if clean_labels is not None:
            cleaner_accuracy = calculate_cleaner_accuracy(predictions_df, pred_col, clean_labels)
        
        results.append({
            'k': k,
            'accuracy': accuracy,
            'cleaner_accuracy': cleaner_accuracy
        })
    
    return pd.DataFrame(results)


def save_knn_accuracy_csv(predictions_df: pd.DataFrame, clean_labels_path: str = None, 
                         directory: str = 'results', filename: str = 'out', 
                         k_values: List[int] = None, project_root=None):
    """
    Calculate and save KNN accuracy results to CSV file.
    
    Args:
        predictions_df (pd.DataFrame): DataFrame containing predictions with k_X_pred columns
        clean_labels_path (str, optional): Path to clean labels CSV
        directory (str): Directory to save the CSV file
        filename (str): Base filename for the CSV file
        k_values (List[int], optional): List of k values to calculate accuracy for
        project_root: Path to project root. If None, will be inferred from this file's location
    
    Returns:
        pd.DataFrame: The accuracy results DataFrame
    """
    if project_root is None:
        project_root = Path(__file__).parent.parent
    
    # Calculate KNN accuracies
    accuracy_results = calculate_knn_accuracy(predictions_df, clean_labels_path, k_values)
    
    # Save to CSV
    if directory.startswith('eval/'):
        save_dir = project_root / directory
    else:
        save_dir = project_root / 'eval' / directory
    save_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = save_dir / f'{filename}_accuracy.csv'
    accuracy_results.to_csv(output_path, index=False)
    print(f'KNN accuracy results saved to: {output_path}')
    
    return accuracy_results


def process_few_shot_results(base_dir: str, m_sample: int, clean_labels_path: str = None, 
                           k_values: List[int] = None, project_root=None):
    """
    Process few-shot results and calculate confidence intervals for different k values.
    
    Args:
        base_dir (str): Base directory containing the few-shot results
        m_sample (int): Number of samples per class (directory name)
        clean_labels_path (str, optional): Path to clean labels CSV
        k_values (List[int], optional): List of k values to process
        project_root: Path to project root. If None, will be inferred from this file's location
    
    Returns:
        pd.DataFrame: Results with confidence intervals
    """
    import math
    import re
    from collections import defaultdict
    from tqdm import tqdm
    
    if project_root is None:
        project_root = Path(__file__).parent.parent
    
    # Default k values similar to the original implementation
    if k_values is None:
        k_values = [1, 3, 5, 7, 9, 11, 13, 21, 49, 51]
    
    # Filter k values based on m_sample limit
    valid_k = [k for k in k_values if k <= m_sample]
    
    # Path to the few-shot results directory
    results_path = Path(base_dir) / str(m_sample)
    if not results_path.exists():
        raise FileNotFoundError(f"Directory {results_path} not found.")
    
    # Load clean labels if provided
    clean_labels = None
    if clean_labels_path and os.path.exists(clean_labels_path):
        try:
            clean_labels = pd.read_csv(clean_labels_path)
        except Exception as e:
            print(f"Warning: Could not load clean labels from {clean_labels_path}: {e}")
    
    # Collect accuracies for each k value across all CSV files
    k_accuracies = defaultdict(list)
    k_clean_accuracies = defaultdict(list)
    csv_count = 0
    
    print(f"Processing few-shot results from: {results_path}")
    
    for csv_file in tqdm(results_path.glob("*.csv"), desc="Processing CSVs"):
        try:
            df = pd.read_csv(csv_file)
            csv_count += 1
            
            if "original_label" not in df.columns:
                raise ValueError(f"Missing 'original_label' column in {csv_file}")
            
            # Calculate regular accuracy for each k value
            for k in valid_k:
                pred_col = f'k_{k}_pred'
                if pred_col not in df.columns:
                    print(f"Warning: Column {pred_col} not found in {csv_file}, skipping k={k}")
                    continue
                
                # Regular accuracy
                valid_predictions = df[df[pred_col].notna()]
                if len(valid_predictions) > 0:
                    acc = 100 * (valid_predictions[pred_col] == valid_predictions["original_label"]).mean()
                    k_accuracies[k].append(acc)
                
                # Clean accuracy if clean labels are available
                if clean_labels is not None:
                    try:
                        merged = pd.merge(clean_labels, df, left_on='id', right_on='img_id')
                        merged[pred_col] = merged[pred_col].astype(str)
                        
                        valid_clean = merged[merged[pred_col].notna()]
                        if len(valid_clean) > 0:
                            valid_clean['correct_clean'] = valid_clean.apply(
                                lambda row: row[pred_col].strip() in str(row['proposed_labels']).strip().split(", ")
                                if "," in str(row['proposed_labels'])
                                else row[pred_col].strip() == str(row['proposed_labels']).strip(),
                                axis=1
                            )
                            clean_acc = valid_clean['correct_clean'].mean() * 100
                            k_clean_accuracies[k].append(clean_acc)
                    except Exception as e:
                        print(f"Warning: Could not calculate clean accuracy for {csv_file}, k={k}: {e}")
                        
        except Exception as e:
            print(f"Error reading {csv_file}: {e}")
    
    print(f'Found {csv_count} CSV files in {results_path}.')
    
    # Calculate confidence intervals
    results = []
    for k in valid_k:
        accs = k_accuracies[k]
        clean_accs = k_clean_accuracies[k]
        
        n = len(accs)
        print(f"Number of accuracies for k={k}: {n}")
        
        if n == 0:
            continue
            
        # Regular accuracy statistics
        mean_acc = sum(accs) / n if n > 0 else 0.0
        std_dev = pd.Series(accs).std(ddof=1) if n > 1 else 0.0
        ci_95 = 1.96 * (std_dev / math.sqrt(n)) if n > 1 else 0.0
        
        result_row = {
            "k": k,
            "average_accuracy": round(mean_acc, 2),
            "std_deviation": round(std_dev, 4),
            "ci_95": round(ci_95, 4),
            "n_experiments": n
        }
        
        # Clean accuracy statistics if available
        if clean_accs and len(clean_accs) > 0:
            n_clean = len(clean_accs)
            mean_clean_acc = sum(clean_accs) / n_clean
            std_dev_clean = pd.Series(clean_accs).std(ddof=1) if n_clean > 1 else 0.0
            ci_95_clean = 1.96 * (std_dev_clean / math.sqrt(n_clean)) if n_clean > 1 else 0.0
            
            result_row.update({
                "average_clean_accuracy": round(mean_clean_acc, 2),
                "std_deviation_clean": round(std_dev_clean, 4),
                "ci_95_clean": round(ci_95_clean, 4),
                "n_clean_experiments": n_clean
            })
        
        results.append(result_row)
    
    return pd.DataFrame(results)


def save_few_shot_summary(base_dir: str, m_sample: int, clean_labels_path: str = None, 
                         k_values: List[int] = None, project_root=None):
    """
    Process few-shot results and save summary with confidence intervals.
    
    Args:
        base_dir (str): Base directory containing the few-shot results
        m_sample (int): Number of samples per class
        clean_labels_path (str, optional): Path to clean labels CSV
        k_values (List[int], optional): List of k values to process
        project_root: Path to project root. If None, will be inferred from this file's location
    
    Returns:
        str: Path to the saved summary file
    """
    if project_root is None:
        project_root = Path(__file__).parent.parent
    
    # Process the results
    results_df = process_few_shot_results(base_dir, m_sample, clean_labels_path, k_values, project_root)
    
    # Save summary to the few_shot directory (not in the m_sample subdirectory)
    save_dir = Path(base_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    summary_path = save_dir / f'{m_sample}_shot_accuracy_summary.csv'
    results_df.to_csv(summary_path, index=False)
    
    print(f'Few-shot summary saved to: {summary_path}')
    print(f'Summary includes {len(results_df)} k values with confidence intervals')
    
    return str(summary_path)
